{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f2b57d",
   "metadata": {},
   "source": [
    "## Setting up the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6342a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42dddd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api_key = config.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12df389",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691da4eb",
   "metadata": {},
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916abc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci-002\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=10,\n",
    "        temperature=0.7)\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e99cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb813243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time , I was a small business owner. I was\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(prompt)\n",
    "print(prompt, generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ff65b",
   "metadata": {},
   "source": [
    "## Customizing the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6815d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens, temperature):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci-002\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature)\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372acc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time , there was a little girl who was born with a very special gift. She could see things that others could not. She could see the future, and she could see the past. She could see the present, and she could see the future.\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(prompt, 50, 0)\n",
    "print(prompt, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9295bd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a young Spokane, David Boucher. You’d see him everywhere and he didn’t want to be looked at until you did and then he’d smile and make you feel better. You’d say, “David, we have a Dennis H\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(prompt, 50, 1)\n",
    "print(prompt, generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554f553",
   "metadata": {},
   "source": [
    "## Summarising Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0fc98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarizer(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You will be provided with a block of text, and your task is to extract a list of keywords from it.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"A flying saucer seen by a guest house, a 7ft alien-like figure coming out of a hedge and a \\\"cigar-shaped\\\" UFO near a school yard.\\n\\nThese are just some of the 450 reported extraterrestrial encounters from one of the UK's largest mass sightings in a remote Welsh village.\\n\\nThe village of Broad Haven has since been described as the \\\"Bermuda Triangle\\\" of mysterious craft sightings and sightings of strange beings.\\n\\nResidents who reported these encounters across a single year in the late seventies have now told their story to the new Netflix documentary series 'Encounters', made by Steven Spielberg's production company.\\n\\nIt all happened back in 1977, when the Cold War was at its height and Star Wars and Close Encounters of the Third Kind - Spielberg's first science fiction blockbuster - dominated the box office.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"flying saucer, guest house, 7ft alien-like figure, hedge, cigar-shaped UFO, school yard, extraterrestrial encounters, UK, mass sightings, remote Welsh village, Broad Haven, Bermuda Triangle, mysterious craft sightings, strange beings, residents, single year, late seventies, Netflix documentary series, Steven Spielberg, production company, 1977, Cold War, Star Wars, Close Encounters of the Third Kind, science fiction blockbuster, box office.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Each April, in the village of Maeliya in northwest Sri Lanka, Pinchal Weldurelage Siriwardene gathers his community under the shade of a large banyan tree. The tree overlooks a human-made body of water called a wewa – meaning reservoir or \\\"tank\\\" in Sinhala. The wewa stretches out besides the village's rice paddies for 175-acres (708,200 sq m) and is filled with the rainwater of preceding months.    \\n\\nSiriwardene, the 76-year-old secretary of the village's agrarian committee, has a tightly-guarded ritual to perform. By boiling coconut milk on an open hearth beside the tank, he will seek blessings for a prosperous harvest from the deities residing in the tree. \\\"It's only after that we open the sluice gate to water the rice fields,\\\" he told me when I visited on a scorching mid-April afternoon.\\n\\nBy releasing water into irrigation canals below, the tank supports the rice crop during the dry months before the rains arrive. For nearly two millennia, lake-like water bodies such as this have helped generations of farmers cultivate their fields. An old Sinhala phrase, \\\"wewai dagabai gamai pansalai\\\", even reflects the technology's centrality to village life; meaning \\\"tank, pagoda, village and temple\\\".\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"April, Maeliya, northwest Sri Lanka, Pinchal Weldurelage Siriwardene, banyan tree, wewa, reservoir, tank, Sinhala, rice paddies, 175-acres, 708,200 sq m, rainwater, agrarian committee, coconut milk, open hearth, blessings, prosperous harvest, deities, sluice gate, rice fields, irrigation canals, dry months, rains, lake-like water bodies, farmers, cultivate, Sinhala phrase, technology, village life, pagoda, temple.\"\n",
    "        }, \n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": prompt\n",
    "        }\n",
    "      ],\n",
    "      temperature=0.5,\n",
    "      max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "272cc352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Reef Guide Kirsty Whitman didn't need to tell me twice. Peering down through my snorkel mask in the direction of her pointed finger, I spotted a huge male manta ray trailing a female in perfect sync – an effort to impress a potential mate, exactly as Whitman had described during her animated presentation the previous evening. Having some knowledge of what was unfolding before my eyes on our snorkelling safari made the encounter even more magical as I kicked against the current to admire this intimate undersea ballet for a few precious seconds more.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Master Reef Guide Kirsty Whitman didn't need to tell me twice. Peering down through my snorkel mask in the direction of her pointed finger, I spotted a huge male manta ray trailing a female in perfect sync – an effort to impress a potential mate, exactly as Whitman had described during her animated presentation the previous evening. Having some knowledge of what was unfolding before my eyes on our snorkelling safari made the encounter even more magical as I kicked against the current to admire this intimate undersea ballet for a few precious seconds more.\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed81104f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Master Reef Guide, Kirsty Whitman, snorkel mask, male manta ray, female, impress, potential mate, animated presentation, snorkelling safari, encounter, magical, current, admire, undersea ballet.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summarizer(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189025b",
   "metadata": {},
   "source": [
    "## Poetic Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c00bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poetic_chatbot(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a poetic chatbot.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"When was Google founded?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"In the late '90s, a spark did ignite, Google emerged, a radiant light. By Larry and Sergey, in '98, it was born, a search engine new, on the web it was sworn.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Which country has the youngest president?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Ah, the pursuit of youth in politics, a theme we explore. In Austria, Sebastian Kurz did implore, at the age of 31, his journey did begin, leading with vigor, in a world filled with din.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = 1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6041f72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the mists of time, an art was found, centuries ago, the making of cheese did abound. By ancient hands, with milk pure and true, curds formed and whey bid adieu. From lands far and wide, this delicacy did spread, a savory creation, in history it has led.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"When was cheese first made?\"\n",
    "poetic_chatbot(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac09b08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a realm where knowledge thrives and grows, the next course on 365DataScience soon shows. With precision and clarity, it will appear, guiding learners far and near. Check the platform for the latest release, a treasure trove of wisdom to increase.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the next course to be uploaded to 365DataScience?\"\n",
    "poetic_chatbot(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afa683",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90710771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722f492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://365datascience.com/upcoming-courses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fc8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96219fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368905d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb11a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "822e2065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/langchain/vectorstores/base.py:417\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    415\u001b[39m texts = [d.page_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    416\u001b[39m metadatas = [d.metadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/langchain/vectorstores/faiss.py:602\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m    577\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m     **kwargs: Any,\n\u001b[32m    583\u001b[39m ) -> FAISS:\n\u001b[32m    584\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m    585\u001b[39m \n\u001b[32m    586\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    600\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m    604\u001b[39m         texts,\n\u001b[32m    605\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    609\u001b[39m         **kwargs,\n\u001b[32m    610\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/langchain/embeddings/openai.py:490\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[32m    479\u001b[39m \n\u001b[32m    480\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    486\u001b[39m \u001b[33;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m    487\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/langchain/embeddings/openai.py:374\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size)\u001b[39m\n\u001b[32m    371\u001b[39m     _iter = \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens), _chunk_size)\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     response = \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     batched_embeddings.extend(r[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    381\u001b[39m results: List[List[List[\u001b[38;5;28mfloat\u001b[39m]]] = [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts))]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/langchain/embeddings/openai.py:107\u001b[39m, in \u001b[36membed_with_retry\u001b[39m\u001b[34m(embeddings, **kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m     response = embeddings.client.create(**kwargs)\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _check_response(response, skip_empty=embeddings.skip_empty)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_embed_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/tenacity/__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/tenacity/__init__.py:418\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    416\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/tenacity/__init__.py:185\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/langchain/embeddings/openai.py:104\u001b[39m, in \u001b[36membed_with_retry.<locals>._embed_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     response = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _check_response(response, skip_empty=embeddings.skip_empty)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/openai/api_resources/embedding.py:33\u001b[39m, in \u001b[36mEmbedding.create\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m         \u001b[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[32m     36\u001b[39m         \u001b[38;5;66;03m# This is only for the default case.\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_provided_encoding_format:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[39m, in \u001b[36mEngineAPIResource.create\u001b[39m\u001b[34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m     **params,\n\u001b[32m    137\u001b[39m ):\n\u001b[32m    138\u001b[39m     (\n\u001b[32m    139\u001b[39m         deployment_id,\n\u001b[32m    140\u001b[39m         engine,\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m         api_key, api_base, api_type, api_version, organization, **params\n\u001b[32m    151\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     response, _, api_key = \u001b[43mrequestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[32m    165\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[39m, in \u001b[36mAPIRequestor.request\u001b[39m\u001b[34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    278\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    279\u001b[39m     method,\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    287\u001b[39m ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    288\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.request_raw(\n\u001b[32m    289\u001b[39m         method.lower(),\n\u001b[32m    290\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m         request_timeout=request_timeout,\n\u001b[32m    297\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     resp, got_stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m.api_key\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[39m, in \u001b[36mAPIRequestor._interpret_response\u001b[39m\u001b[34m(self, result, stream)\u001b[39m\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    693\u001b[39m         \u001b[38;5;28mself\u001b[39m._interpret_response_line(\n\u001b[32m    694\u001b[39m             line, result.status_code, result.headers, stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    695\u001b[39m         )\n\u001b[32m    696\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result.iter_lines())\n\u001b[32m    697\u001b[39m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    706\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    707\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/llms_course_env/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[39m, in \u001b[36mAPIRequestor._interpret_response_line\u001b[39m\u001b[34m(self, rbody, rcode, rheaders, stream)\u001b[39m\n\u001b[32m    763\u001b[39m stream_error = stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp.data\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m200\u001b[39m <= rcode < \u001b[32m300\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle_error_response(\n\u001b[32m    766\u001b[39m         rbody, rcode, resp.data, rheaders, stream_error=stream_error\n\u001b[32m    767\u001b[39m     )\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[31mRateLimitError\u001b[39m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab290f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(openai_api_key=api_key, \n",
    "                                                  model=\"gpt-3.5-turbo\", \n",
    "                                                  temperature=0), \n",
    "                                           vectorstore.as_retriever(), \n",
    "                                           memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1520ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the next course to be uploaded on the 365DataScience platform?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29986146",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d181fc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next course to be uploaded on the 365DataScience platform is \"LLM Engineering in Practice with Streamlit and OpenAI\" with Petar Petrov. It is a hands-on course that explores the world of AI solutions, focusing on designing, developing, and deploying an interview simulator project using Streamlit.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d911ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_course_env",
   "language": "python",
   "name": "llms_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
